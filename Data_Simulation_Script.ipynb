{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a79f1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307317b",
   "metadata": {},
   "source": [
    "## Simulation Process\n",
    "\n",
    "To simulate incremental data ingestion into a logical data warehouse, the dataset will be split and processed in two phases:\n",
    "- A full initial load\n",
    "- Daily incremental loads that represent future data updates and inserts\n",
    "\n",
    "**Phase 1: Splitting Old and New Data**\n",
    "\n",
    "The cutoff date is December 31, 2019. Any record with a `ValidFrom` on or before 2019-12-31 is considered historical and will be included in the initial full load. Records with a `ValidFrom` after 2019-12-31 represent new data that will be incrementally ingested.\n",
    "\n",
    "Although all source data exists prior to the current date, for simulation purposes, dates from January 1, 2020, onward will be treated as future incremental loads.\n",
    "\n",
    "Steps:\n",
    "- Identify dimension tables that contain records spanning both before and after the cutoff date\n",
    "- Split these tables into: initial \"old\" data and '\"new\" data\n",
    "- For customers and suppliers tables, merge their new data, if any, with the corresponding simulated update tables (described below)\n",
    "- Partition all \"new\" data by `ValidFrom`, producing one partition (folder/file) per date. Each partition represents the data arriving on that day, to be ingested sequentially\n",
    "\n",
    "**Phase 2: Simulating data updates for existing records**\n",
    "\n",
    "Customers:\n",
    "- Randomly select 200 existing customers (from the \"old\" dataset).\n",
    "- Update the following fields:\n",
    "    - `CreditLimit`\n",
    "    - `StandardDiscountPercentage`\n",
    "    - `ValidFrom`\n",
    "- The new `ValidFrom` date is randomly assigned between 2020-01-01 and 2021-04-17 (the end of the available data)\n",
    "- These updated records are then partitioned by their new  `ValidFrom` date for daily ingestion\n",
    "\n",
    "Suppliers:\n",
    "- Randomly select a set of existing suppliers (sample size and set can vary for each field generation).\n",
    "- Update the following fields:\n",
    "    - `PaymentDays`\n",
    "    - `BankAccountName`\n",
    "    - `PhoneNumber`\n",
    "    - `ValidFrom`\n",
    "- The `ValidFrom` date is again randomly selected within 2020-01-01 to 2021-04-17.\n",
    "- Updated supplier records are also partitioned daily by `ValidFrom` for ingestion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tables with validfrom values potentially after 12/31/2019, the last day of old data in simulation:\n",
    "csv.field_size_limit(10_000_000)\n",
    "all_customers = pd.read_csv(\n",
    "    \"Downloaded_Initial/Sales_Customers/Sales_Customers.csv\",\n",
    "    delimiter=\"|\",\n",
    "    quoting=3,  # 3 = csv.QUOTE_NONE = keep all \"\" marks as literal\n",
    "    engine=\"python\",\n",
    "    escapechar='\\\\'  # optional, just in case\n",
    ")\n",
    "\n",
    "all_cities = pd.read_csv(\n",
    "    \"Downloaded_Initial/Application_Cities/Application_Cities.csv\",\n",
    "    delimiter=\"|\",\n",
    "    quoting=3,\n",
    "    engine=\"python\",\n",
    "    escapechar='\\\\'\n",
    ")\n",
    "\n",
    "all_countries = pd.read_csv(\n",
    "    \"Downloaded_Initial/Application_Countries/Application_Countries.csv\",\n",
    "    delimiter=\"|\",\n",
    "    quoting=3,\n",
    "    engine=\"python\",\n",
    "    escapechar='\\\\'\n",
    ")\n",
    "\n",
    "all_stateprov = pd.read_csv(\n",
    "    \"Downloaded_Initial/Application_StateProvinces/Application_StateProvinces.csv\",\n",
    "    delimiter=\"|\",\n",
    "    quoting=3,\n",
    "    engine=\"python\",\n",
    "    escapechar='\\\\'\n",
    ")\n",
    "\n",
    "# load in suppliers\n",
    "all_suppliers = pd.read_csv(\n",
    "    \"Downloaded_Initial/Purchasing_Suppliers/Purchasing_Suppliers.csv\",\n",
    "    delimiter=\"|\",\n",
    "    quoting=3,\n",
    "    engine=\"python\",\n",
    "    escapechar='\\\\'\n",
    ")\n",
    "\n",
    "# load in stock items\n",
    "stock_items = pd.read_csv(\n",
    "    'Downloaded_Initial/Warehouse_StockItems/Warehouse_StockItems.csv',\n",
    "    delimiter=\"|\",\n",
    "    quoting=3, \n",
    "    engine=\"python\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fad387b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_customers:40\n",
      "all_cities:17\n",
      "all_countries:5\n",
      "all_stateprov:8\n",
      "all_suppliers:0\n"
     ]
    }
   ],
   "source": [
    "# find tables that need new, old splits\n",
    "df_tables = {'all_customers': all_customers, 'all_cities': all_cities,\n",
    "             'all_countries': all_countries, 'all_stateprov': all_stateprov,\n",
    "             'all_suppliers': all_suppliers}\n",
    "for name, table, in df_tables.items():\n",
    "    print(f\"{name}:{len(table[table['ValidFrom'] > '2019-12-31'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e5361b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split new and old customers\n",
    "new_customers = all_customers[all_customers['ValidFrom'] > '2019-12-31'].sort_values('CustomerID').reset_index(drop=True)\n",
    "old_customers = all_customers[all_customers['ValidFrom'] <= '2019-12-31'].sort_values('CustomerID').reset_index(drop=True)\n",
    "\n",
    "# split new and old cities\n",
    "new_cities = all_cities[all_cities['ValidFrom'] > '2019-12-31'].sort_values('CityID').reset_index(drop=True)\n",
    "old_cities = all_cities[all_cities['ValidFrom'] <= '2019-12-31'].sort_values('CityID').reset_index(drop=True)\n",
    "\n",
    "# split new and old countries\n",
    "new_countries = all_countries[all_countries['ValidFrom'] > '2019-12-31'].sort_values('CountryID').reset_index(drop=True)\n",
    "new_countries.drop(columns=['Border'], inplace=True) # dropping unneeded long geo border polygon column\n",
    "old_countries = all_countries[all_countries['ValidFrom'] <= '2019-12-31'].sort_values('CountryID').reset_index(drop=True)\n",
    "old_countries.drop(columns=['Border'], inplace=True)\n",
    "\n",
    "# split new and old state provinces\n",
    "new_stateprov = all_stateprov[all_stateprov['ValidFrom'] > '2019-12-31'].sort_values('StateProvinceID').reset_index(drop=True)\n",
    "new_stateprov.drop(columns=['Border'], inplace=True)\n",
    "old_stateprov = all_stateprov[all_stateprov['ValidFrom'] <= '2019-12-31'].sort_values('StateProvinceID').reset_index(drop=True)\n",
    "old_stateprov.drop(columns=['Border'], inplace=True)\n",
    "\n",
    "# minor cleaning of stock items table\n",
    "stock_items['StockItemName'] = stock_items['StockItemName'].str.replace('\"', '').str.replace('\\\\', '')\n",
    "stock_items['SearchDetails'] = stock_items['SearchDetails'].str.replace('\"', '').str.replace('\\\\', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15adc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random set of updates to occur during new period for existing customers:\n",
    "np.random.seed(42)\n",
    "\n",
    "sample_size = 200\n",
    "update_customers = old_customers.sample(n=200).copy()\n",
    "\n",
    "# function to pick a new value for each row of provided column; new values are picked at random from the column's existing set of values\n",
    "def random_choice_from_column(s, n=sample_size):\n",
    "    unique_values = s.unique()\n",
    "    return np.random.choice(unique_values, size=n)\n",
    "\n",
    "# simulate updates to credit limit\n",
    "update_customers['CreditLimit'] = random_choice_from_column(old_customers['CreditLimit'])\n",
    "\n",
    "# simulate updates standard discount %\n",
    "update_customers['StandardDiscountPercentage'] = random_choice_from_column(old_customers['StandardDiscountPercentage'])\n",
    "\n",
    "# simulate updates to valid from; will serve as the date of the update\n",
    "sim_start = pd.Timestamp('2020-01-01')\n",
    "sim_end = pd.Timestamp('2021-04-17')\n",
    "update_customers['ValidFrom'] = pd.to_datetime(np.random.randint(sim_start.value // 10**9, sim_end.value / 10 ** 9, sample_size),\n",
    "                                               unit='s')\n",
    "update_customers = update_customers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89a22ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random updates to occur during new period for existing suppliers:\n",
    "np.random.seed(42)\n",
    "bank_updates = all_suppliers.copy()\n",
    "\n",
    "# simulate updates to bank account name; about half of suppliers will have a change during the simulation\n",
    "bank_mask = np.random.choice([True, False], size=len(bank_updates))\n",
    "\n",
    "update_bank_names = ['Alpha', 'Beta', 'Delta', 'Gamma', 'Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "bank_updates = bank_updates[bank_mask]\n",
    "bank_updates['BankAccountName'] = bank_updates['BankAccountName'] + ' ' + np.random.choice(update_bank_names,\n",
    "                                                                                           size=len(bank_updates))\n",
    "bank_updates['BankAccountName'] = bank_updates['BankAccountName'].str.replace('\"', '')\n",
    "bank_updates['BankAccountName'] = '\"' + bank_updates['BankAccountName'] + '\"'\n",
    "\n",
    "# function to generate random phone numbers\n",
    "def fake_phone_numbers():\n",
    "    return f'({np.random.randint(200, 999)}) {np.random.randint(100, 999)}-{np.random.randint(1000, 9993)}'\n",
    "\n",
    "# simulate updates to phone number on another random set of suppliers\n",
    "np.random.seed(41)\n",
    "phone_updates = all_suppliers.copy()\n",
    "phone_mask = np.random.choice([True, False], size=len(phone_updates))\n",
    "phone_updates = phone_updates[phone_mask]\n",
    "\n",
    "phone_updates['PhoneNumber'] = [fake_phone_numbers() for i in range(phone_mask.sum())]\n",
    "\n",
    "# simulate updates to payment days on another random set of suppliers\n",
    "np.random.seed(40)\n",
    "pday_updates = all_suppliers.copy()\n",
    "pday_mask = np.random.choice([True, False], size=len(pday_updates))\n",
    "pday_updates = pday_updates[pday_mask]\n",
    "\n",
    "pday_updates['PaymentDays'] = random_choice_from_column(all_suppliers['PaymentDays'], pday_mask.sum())\n",
    "\n",
    "# combine update tables + simulate updates to valid from\n",
    "update_suppliers = pd.concat([bank_updates, phone_updates, pday_updates])\n",
    "update_suppliers['ValidFrom'] = pd.to_datetime(np.random.randint(sim_start.value // 10**9, sim_end.value / 10 ** 9, len(update_suppliers)),\n",
    "                                               unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7705f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine new and update tables for customers, no new data for suppliers\n",
    "new_update_customers = pd.concat([update_customers, new_customers])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "04db20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition new data into different folders and files by validfrom and dimension they represent\n",
    "change_dates = set()\n",
    "for df in [update_suppliers, new_update_customers, new_stateprov, new_cities, new_countries]:\n",
    "   change_dates |= set(pd.to_datetime(df['ValidFrom']).dt.date)\n",
    "\n",
    "change_dates = sorted(change_dates)\n",
    "\n",
    "change_tables = {'Purchasing_Suppliers': update_suppliers, 'Application_Cities': new_cities,\n",
    "                 'Application_Countries': new_countries, 'Application_StateProvinces': new_stateprov,\n",
    "                 'Sales_Customers': new_update_customers}\n",
    "\n",
    "\n",
    "for date in change_dates:\n",
    "   date_path = f\"Changed_Data/{date.strftime('%Y-%m-%d')}\"\n",
    "   os.makedirs(date_path, exist_ok=True)\n",
    "\n",
    "   for name, table in change_tables.items():\n",
    "      daily_load = table[pd.to_datetime(table['ValidFrom']).dt.date == date]\n",
    "\n",
    "      if not daily_load.empty:\n",
    "         category_path = os.path.join(date_path, name)\n",
    "         os.makedirs(category_path, exist_ok=True)\n",
    "\n",
    "         file_path = os.path.join(category_path, f\"{name}_{date.strftime('%Y-%m-%d')}.csv\")\n",
    "         daily_load.to_csv(file_path,\n",
    "                      index=False,\n",
    "                      sep='|',\n",
    "                      quoting=csv.QUOTE_NONE,\n",
    "                      escapechar=None)\n",
    "\n",
    "\n",
    "# place old data into relevant directory for initial loads\n",
    "old_tables = {'Application_Cities': old_cities,'Application_Countries': old_countries,\n",
    "              'Application_StateProvinces': old_stateprov, 'Sales_Customers': old_customers,\n",
    "              'Warehouse_StockItems': stock_items}\n",
    "\n",
    "for name, table in old_tables.items():\n",
    "   path = f'Simulation_Initial_Dims/{name}'\n",
    "   os.makedirs(path, exist_ok=True)\n",
    "\n",
    "   file_path = os.path.join(path, f\"{name}.csv\")\n",
    "   table.to_csv(file_path,\n",
    "                index=False,\n",
    "                sep='|',\n",
    "                quoting=csv.QUOTE_NONE,\n",
    "                escapechar=None)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
